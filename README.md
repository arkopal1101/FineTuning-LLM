# Fine-Tuning LLMs with LoRA on SQuAD

## ðŸ“Œ Overview
This project demonstrates how to **fine-tune FLAN-T5 (Large)** on the **SQuAD v1.1 dataset** using **LoRA (Low-Rank Adaptation)** for efficient parameter tuning.  
It showcases how to adapt large language models with minimal compute while still achieving strong performance on extractive question-answering tasks.  

The notebook also includes **comprehensive evaluation metrics** such as BLEU, ROUGE, METEOR, GLEU, Repetition Rate, Flesch Reading Ease, CoSIM, BERTScore, Toxicity, Novelty, and Diversity.

---

